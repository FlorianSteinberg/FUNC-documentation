\documentclass{article}

\usepackage[utf8]{inputenc}
%\usepackage[ngerman]{babel}
\usepackage{xspace,listings,color}
\usepackage[colorlinks=true]{hyperref}
\usepackage{amsmath, amssymb}
\usepackage{cleveref}

\definecolor{commentgray}{rgb}{0.4,0.4,0.4}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{keywordred}{rgb}{0.7,0.1,0.1}
\definecolor{stringyellow}{rgb}{0.7,0.5,0.82}
\lstset{backgroundcolor=\color{white},
	basicstyle=\footnotesize,     
	breakatwhitespace=false,      
	breaklines=true,              
	captionpos=b,                 
	commentstyle=\color{commentgray}, 
	deletekeywords={...},         
	escapeinside={\%*}{*)},       
	keywordstyle=\color{keywordred},    
	language=C++,              
	morekeywords={*,...},         
	numbers=left,                 
	numbersep=5pt,                
	numberstyle=\tiny\color{gray},
	rulecolor=\color{black},      
	showspaces=false,             
	showstringspaces=false,       
	showtabs=false,               
	stringstyle=\color{stringyellow},
	tabsize=2,
	belowskip=.25cm}

\DeclareMathOperator{\NN}{\ensuremath{\mathbb{N}}\xspace}
\DeclareMathOperator{\RR}{\ensuremath{\mathbb{R}}\xspace}
\DeclareMathOperator{\CC}{\ensuremath{\mathbb{C}}\xspace}

\newcommand{\irram}{\texttt{iRRAM}\xspace}
\newcommand{\irrams}{\texttt{iRRAM}s\xspace}
\newcommand{\cc}{\texttt{C++}\xspace}
\newcommand{\ccOx}{\texttt{C++11}\xspace}
\newcommand{\ir}[1]{\texttt{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\func}[1]{\texttt{#1}}
\newcommand{\NULL}{\texttt{NULL} pointer\xspace}
\newcommand{\temp}[1]{\textcolor{red}{#1}}
\title{A type for Taylor series for the \cc library \irram for exact real arithmetic}
\date{last edited \today}
\author{Florian Steinberg}

\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage
\section*{Introduction}

We aim to implement a type of analytic functions in \irram. The \irram is a \cc package for error-free real arithmetic. We will begin by summarizing the facts about computable analytic functions we need. We will go on to describe some of the key features of \irram. The we will present some parts of the \ccOx standard template library we need for implementation. Finally we will describe how the implementation was done. In the last chapter we will address some shortcomings and possible future improvements.

\part{Higher type computability theory}

\section{Computable analytic functions}

We will only consider analytic functions, that are equal to their Taylor expansion around zero on a open superset of the Unit disc. This means, that we can find some $r>1$ which is still strictly smaller than the radius of convergence. In the following we will fix such a function $f$.

$f$ is uniquely determined by its Taylor-series in zero. In the following, we will denote the Taylor-series of $f$ by $(a_n)_{n\in \NN}$. It can be expressed in terms of $f$s derivatives, or by Cauchy's differentiation formula:
\[ a_n = \frac{f^{(n)}(0)}{n!} = \frac 1 {2\pi i}\int_{|z| = r} \frac{f(z)}{|z|^{n+1}} d\lambda. \]
The sequence $(a_n)_{n\in \NN}$ is computable (as sequence of real numbers) if and only if $f$ is computable (as continuous function). 

\subsection{The constants $k$ and $A$}

Unfortunately it is not possible to evaluate such a function effectively without further information. We additionally need two constants $k$ and $A$. $k$ will be such, that $r:=\sqrt[k]{2}$ is still smaller than the radius of convergence of $(a_n)_{n\in \NN}$ and $A$ such that for all $n\in \NN$
\[ |a_n|  r^n \leq A. \]
Since we demanded a convergence radius strictly larger than 1, constants like this will always exist.

We will briefly discuss how these constants can be found: Since the radius of convergence of $f$ is assumed to be strictly larger than $1$, and $r:=\sqrt[k] 2$ goes to $1$ as $k$ goes to infinity, it is possible to choose $k$ big enough for $r$ to be in between $1$ and the radius of convergence. Now fix such an $k$. Consider the function
\[ f|_{\{z:|z| = r\}}. \]
Since this is a continuous function on a compact domain, it will be bounded. If $A$ is a bound of this function, the Cauchy differentiation formula gives:
\[ |a_n |=  \left|\frac 1 {2\pi i}\int_{|z| = r} \frac{f(z)}{|z|^{n+1}} d\lambda\right| \leq \frac A {r^n} \]
Thus $A$ is as desired.

Using these constants, we can obtain a tail estimate:
\[ \left|\sum_{n \geq N} a_n z^n\right| \leq A \frac{(|z|/r)^N}{1- |z|/r}. \]
In particular we get a bound for $f$ on the unit disc: If $|z|\leq 1$, we get:
\[ |f(z)| \leq A\frac r{r-1}. \]
More information on how to calculate these constants (for example for the product of two such functions) can be found in \ref{}. The most important part of the source for our intentions will be Theorem 3.3. The proof of this Theorem in particular specifies how constants $k'$ and $A'$ for the derivative $f'$ can be found, namely:
\[ k' := 2 k \]
and
\[ A' := \left\lceil \frac{A}{r} \left(1+ \frac{2k}{e \ln(2)}\right)\right\rceil. \]
Since any bound of $f'$ on the unit disc is also a bound of the Lipschitz constant of $f$, this allows us to explicitly calculate a Lipschitz constant
\[ L := \left\lceil A \frac{\left(1 + \frac{2k}{e\ln(2)}\right)}{r-\sqrt{r}}\right\rceil \]
for $f$ with very little computational effort.

\temp{We have used the ceiling function above. In terms of computable analysis, this function is not well behaved (since it is not continuous, it will not be computable). Thus we will use the function $x \mapsto \ir{round2}(x) +1$ as a replacement. Here \ir{round2}() is a multivalued function already implemented in \irram.}

\part{\irram}

In this part, we are going to describe some key features of the \cc library \irram for exact real arithmetic. We do not claim our depiction to be complete or self-contained. Instead we restrict ourselves to those parts of \irram that are of importance for us, and can neither be found in \cite{} nor in \cite{} \temp{yet?}.

\section{Basic functionalities}

\subsection{The general idea}

At first glance the most reasonable approach to computable analysis would be to represent a real number $x$ by an algorithm $P$ taking a natural number $n$ and returning an approximation, i.e. an appropriately encoded rational number $x_n$ such that $|x-x_n|<2^{-n}$. This kind of proceeding bears the following problems:
\begin{itemize}
\item Each time a sum or a product of real numbers is calculated, the algorithms of the corresponding real numbers must be copied or at least referenced. This leads to a tree-like structure of any program and often to uncontrollable growth of memory consumption.
\item A algorithm $P$ encoding a real number carries more information than the real number itself. If $P$ is given to a function, this function can for example also use the running time of $P$ to generate its output. This kind of functions should from a mathematical point of view not be considered computable, and can lead to pathological behavior.
\end{itemize}

Thus the \irram chooses a different approach. Namely the real numbers are represented by finite intervals containing said real number. All manipulations are carried out with these intervals. If a situation occurs, where the precision is simply not sufficient anymore, the whole calculation is restarted with higher precision. This procedure is called a \emph{reiteration}. The single runs of the program with different precisions will be referred to as \emph{iterations}.

At first glance, this approach seems to be very time consuming: The whole computation might be restarted repeatedly, discarding all the computations made in the earlier iterations completely. But it is well known, that this does not blow up the asymptotic complexity: More precisely, the asymptotic complexity of the whole computation and the last reiteration coincide \cite{}.

\irrams approach does not suffer the problems listed above, but brings its own (which are hopefully more manageable):
\begin{itemize}
\item If the program includes in and output, a restart of the program will lead to doubled output. This is mainly a implementation problem and can be avoided by using the output methods provided by \irram.
\item If on the other hand, the program asked the user for input at some point, this input will have to be memorized. Moreover: if any multivalued function is computed, it has to evaluate to the exact same value in the next run and has to be memorized. This is to avoid incoherences in output.
\item Each time a reiteration is triggered, the whole program restarts. This means, that nearly everything (as mentioned above some things are memorized) is re-evaluated. In particular: If the program is composed of two tasks, and one of those needs a higher precision, both tasks will be carried out in this higher precision. This means, that reiterations are very expensive in terms of computation time.
\end{itemize}

The problem described in the last bullet can partially be eliminated by dividing the tasks into two sub-programs. However, as mentioned above, this does not effect the asymptotic running time, and we thus will not go into it in more detail. Instead we will go into the problem discussed in the first bullet in more detail in the oncoming section. \temp{Note, that the content of the remaining bullet is very closely related. It may not be addressed on its own.}

Another point is, that programs for \irram are written in \cc. Since \cc is a very powerful programming language, it is often possible for the user to do things he is not really supposed to do.

\subsection{Output}

We did already mention, that output can be a problem and that one should use the output methods provided by \irram. To cope with the difficulties of duplicated output, \irram has its own class of output streams \irram::\ir{orstream}, which replaces the standard output streams. The following can be used for output:
\begin{itemize}
\item Any \irram::\ir{orstream} via the overloaded \ir{<<} operators. In particular the standard one \irram::\ir{cout}.
\item The functions \irram::\ir{rwrite}, \irram::\ir{rwritee} and \irram::\ir{rshow}.
\item The function \irram::\ir{rfprint}.
\end{itemize}
In the following we will leave the preceding \irram::\ir{} away. This coincides with the syntax used inside of \irram programs. One should note, that \ir{cout} differs from \code{std::cout}: For example pointers will be output as $1$ if they point somewhere and as $0$ if they do not. This is important, since pointer addresses can change in reiterations, which could lead to inconsistent output. Additionally \ir{cout} might change the path of computation, since it can trigger reiterations and there are situations where \ir{cout} does not lead to output at all (for example in passages where reiterations might be triggered). Thus when changing \irram itself, it is sometimes a good idea to still use \code{std::cout} for debugging purposes.

The first point in the list above does play a special role here, since it is used by all the others. They call the function \ir{swrite}, which prints a \ir{REAL} to a string and then output this value through the \ir{cout} by using the \ir{<<} operator.

Thus we will first take a closer look at the class \ir{orstream}. The main components of an \ir{orstream} are:
\begin{description}
\item[\code{target}:] An \code{std::ostream} pointer, used for output.
\item[\code{requests}:] A static but thread specific request counter.
\item[\code{outputs}:] A static but thread specific output counter.
\item[\code{real\_w}:] A standard value for the output width of real numbers.
\item[\code{real\_f}:] \ref{}.
\end{description}
The output operator \code{<<} is defined for most standard data types of \cc, and for the \irram specific data types like \ir{REAL}, \ir{DYADIC}, \ir{RATIONAL}, \ir{INTEGER} and \ir{COMPLEX} (which will be explained in more detail in \cref{}, or can be found in \cite{}).

Lets assume the operator \code{<<} is called with some \ir{orstream} \code{ors} and some standard \cc data type \code{x}. \code{ors} will then increase the counter \code{requests}, check if \code{requests} is bigger than \code{outputs}, and if so to pass \code{x} to the \ir{std::ostream} pointed at by target and also increase the counter \code{outputs}. If now a reiteration occurs, the counter \code{requests} will be reset, but the counter \code{outputs} will be kept. This leads to the following behaviour: In the next iteration first \code{outputs} outputs will be ignored, these are evidently exactly those which where already printed in one of the earlier iterations.

If the operator \code{<<} is called with some \code{x} of \irram specific data type, then \code{ors} will call the function \ir{swrite} with parameters \code{x} and \ir{real\_w}, which will print \code{x} to a string \code{xs} of length \ir{real\_w}, but at least $9$ characters. Then \code{ors} will call \code{ors << xs}. Since the precision of \code{x} may not suffice to extract a string representing \code{x} of the desired length, \ir{swrite} might trigger a reiteration.

We emphasize once more: It is important to acknowledge the restrictions of the \ir{orstream}s. If you could output pointer addresses, these would change in reiterations. Since output once written will not be revised, these might be wrong in later iterations. In this case the \irram takes care of the problem by not outputting pointer addresses but first casting them to \code{bool}s. \temp{A real example: if you output the error of some \ir{REAL} it might show values greater than one, which might not reflect the future behaviour as the very next command could be to output the REAL, which will then trigger a reiteration and increase precision. Since the output will not be revised the result could be confusing for the user. This is put down in red, since it might be considered a bug and removed in future versions.} Although these problems should be handled by \irram itself, it is clear that there will always be some ways to trick the system and it is advised to handle output with care.

The remaining output functions simply call the function \ir{swrite} with the desired precision and then hand the string to \irrams standard \ir{orstream} \ir{cout}. Thereby bypassing \ir{cout}s standard output precision.


\section{Data types}

\subsection{The class \ir{REAL}}

The data type \ir{REAL} is the heart of \irram. The class models real numbers via Intervals. A overview over the main functionalities can be found in \cite{}. We will take a closer look at the implementation. The most important members of the class \ir{REAL} are:
\begin{description}
\item[bla:] A pointer to an MP object.
\item[blabla:]
\end{description}

\subsection{The class \ir{INTEGER}}

As the name might indicate, the class \ir{INTEGER} is a class to model integers. The main advantage over the \cc type \code{int}, is that there is no restriction on the size of the Integer. The main drawback is, that the size and speed. Thus it should only be used, when an unbounded size is really necessary.

Two examples: We will use the data type \ir{INTEGER} for the constants $k$ and $A$ described in \cref{sec: k and A}, since a reasonable small number of iterated differentiations can lead to a growth of these constants which would lead to an overflow of \code{int}s (or \code{unsigned int}s for that matter). On the other hand, we will model series as maps that map \code{unsigend int}s to \code{REAL}s, since it seems to be very unlikely that we will ever need to access a element of the sequence beyond the 4.294.967.295-th. Especially since we are mostly dealing with effectively convergent series. \temp{This does contradict the purpose of \irram as library for \emph{exact} real Arithmetic, as it leads to the existence of a best possible approximation and the possibility of an overflow (independent of the available memory). We note, that there already exist limitations of this kind in other parts of \irram. For example in \cite{} on page \ref{} a similar restriction is implied. It would be interesting to compare those restrictions in their severity.}

\temp{describe how INTEGER and REAL interact}

\subsection{The class \ir{COMPLEX}}

The complex numbers play a very important role in the theory of the analytic function. This importance will only be partially reflected in our implementation. The reason is that equality is not computable. This means that it can not be checked whether the imaginary part of a complex number vanishes. This entails the following two drawbacks when handling real valued analytic functions as a special case of analytic functions with complex coefficients:
\begin{itemize}
\item Since we can not detect whether a sequence is really real valued, all operations would have to be carried out with the complex series with vanishing imaginary part. This means we would spend a lot of time on unnecessary operations of adding and multiplying zeros.
\item For similar reasons, we would have to output complex numbers upon real input. This can not easily be fixed, since there is no canonical way to convert a complex number to a real (compare \cref{}).
\end{itemize}
We will thus work with real coefficient series, and remark that these can be easily used to implement analytic functions with complex coefficients, since the real and imaginary part are analytic functions with real coefficients.

Nonetheless we will want to allow evaluation of real analytic functions in complex arguments, thus we need a more or less complete type for complex numbers. \irram provides the class \ir{COMPLEX} for this purpose. As we do already have a type for real numbers, the implementation of this type is very simple and need not be further addressed here. However, the type lacks some basic capabilities we will need. For example, the operators \ir{<<} and \code{+=} are not overloaded and there are no constructors from types other than \ir{REAL}. We give a short description of the improvements we made:
\begin{description}
\item[constructors:] We added the following three constructor templates:
\begin{lstlisting}
template<class T>
COMPLEX(const T& real_part);
template<class T, class S>
COMPLEX(const T& real_part, const S& imag_part);
template<class T, class S>
COMPLEX(std::pair<T,S> p);
\end{lstlisting}
The first takes an arbitrary type \code{T}, attempts to convert this type to a \ir{REAL}, and then constructs a \ir{COMPLEX} having the result as real part and imaginary part zero. The second does the same, but for both real and imaginary part. The third constructor will accept a \code{std::pair} instead of two parameters. This is in particular useful, since it enables the use of nested lists to construct vectors of complex numbers:
\begin{lstlisting}
vector<COMPLEX> v = {{1,2}, {pi(),4.5}};
\end{lstlisting}
\item[operators and functions:] The operators \code{+=} and \code{*=} where implemented in the obvious way. Furthermore the two functions
\begin{lstlisting}
COMPLEX power(const COMPLEX&, const COMPLEX&);
COMPLEX power(const COMPLEX&, const int);
\end{lstlisting}
were added. The implementations are slightly adjusted copies of those of the real counterparts.
\item[output:] The operator \ir{<<} was overloaded. The following conventions were established:
\begin{itemize}
\item The precision will be interpreted point wise. This means, both the real and the imaginary part will be printed with the specified precision. This corresponds to the use of the supremum norm on $\CC = \RR^2$.
\item Complex numbers will be coated in parentheses. This is to avoid confusion. For example the attempt to output a linear monomial via a template as follows:
\begin{lstlisting}
template<class T>
void out(const T& x) {
	cout << x << " * X" << endl;
}
\end{lstlisting}
would otherwise lead to ambiguous (or wrong) output.
\item If the imaginary part of the number is smaller than the desired precision, only the real part will be printed (i.e. \code{+1.00E+0001} instead of (\code{+1.00E+0001+ 0 \ \ \ \ \  i})). The same procedure for vanishing real part.
\end{itemize}
\end{description}

\section{\irram functions}

\subsection{Limits}

\subsection{\irram::\ir{FUNCTION}s}

We aim to implement a type for analytic functions in \irram, which is not yet present. But there already is a type of functions implemented. We will give a short review of this type.

\part{Implementation}

\temp{some warming words}

\section{Tools provided by the \ccOx standard template library}

\cc provides four kinds of functional objects: functions, member functions, function pointer and member function pointer. Since the first two are very common, we assume the reader is familiar with them. However, it is important to note that a function pointer can only point at functions that where created outside of the main program. This makes it impossible to dynamically create new functions when needed, which is a serious shortcoming for us, since we want to be able to add and multiply functions.

Member function pointer do actually solve this problem, since classes, and with them their member functions, can be dynamically created and destroyed. Using member functions to achieve the flexibility we desire would be very involved, luckily for us \ccOx added improved syntax for exactly this. In the following sections review these tools.

\subsection{\code{std::function}s}\label{sec: std::functions}

The std::function template is a general-purpose polymorphic function wrapper (according to cppreference.com). We will find \code{std::function}s to be highly useful. But it will be not until we learn about the \cc lambda calculus, that we can grasp their whole potential. Thus the description in this chapter might appear somewhat unspectacular. A \code{std::function} can be defined by
\begin{lstlisting}
std::function<RESULT(PARAM)> f;
\end{lstlisting}

A std::function can be evaluated like a function, and defined from a function pointer, as the following short example shows:
\begin{lstlisting}
#include<functional>

using std::function;

double f(int i) {
	return double(i);
}

int main()
{
	function<double(int)> g = f;
	cout << g(4) << endl;
	return 0;
}
\end{lstlisting}

The \code{std::function} can do a lot more than regular functions though. For example: If \code{f} is a \code{std::function<RESULT(PARAM1, PARAM2)>} of two arguments, we can define a \code{std::function<RESULT(PARAM2)>} \code{g} by setting the first parameter to a fixed value (say \code x) using the function \code{std::bind}:
\begin{lstlisting}
function<RESULT(PARAM2)> g = bind(f, x, std::placeholders::_1);
\end{lstlisting}
To achieve something similar with regular function pointers is complicated (though it is possible). We will see that some of those possibilities hold risks, since \irram might not expect to encounter a function, containing real numbers that are not handed to it as an parameter.

The syntax \code{function<RESULT(PARAM)>} of \code{std::function}s seems nicer than the \ir{FUNCTION<PARAM, RESULT>} we have seen before. It is also more convenient, since confusing parameter and result type gets a lot harder. The former syntax is implemented by a template specialisation. The definition of looks like this:
\begin{lstlisting}
template<class T>
class function {};

template<class T, class S>
class function<T(S)> {/*...*/};
\end{lstlisting}
Here the first two lines define an empty template class, then lines four and five specialize the definition in the case, that the template argument is of a specific form. Namely a string of the form T(S), where T and S are some arbitrary types. We will use this trick to also improve the syntax of our function type.

\code{std::function}s are in many respects superior to \cc functions, but they lack one thing functions do have: the possibility to be made templates.


\subsection{The \cc lambda calculus}\label{sec: The cc lambda calculus}

One of the main sources for \code{std::functions} is the \cc lambda calculus. A lambda expression in \cc is of the form
\begin{lstlisting}
[/*captures*/](/*parameters*/) -> /*output_type*/ {/*algorithm*/};
\end{lstlisting}
Where the commented parts are to be replaced as follows:
\begin{description}
\item[\textcolor{commentgray}{\code{/*captures*/}}] is to be replaced by a list of variables of local scope, that are needed by the algorithm but supposed to appear as parameters of the function (those are actually what mathematicians mean, when they say \lq parameters\rq). A variable occurring in this list will be called captured (by the lambda function). Global variables need not be captured and can be accessed from the algorithm anyway. Variables may be captured by copy or, if they are preceded by an \lq\code{\&}\rq, by reference.
\item[\textcolor{commentgray}{\code{/*parameters*/}}] is to be replaced by the list of parameters of the function to be constructed.
\item[\textcolor{commentgray}{\code{/*output\_type*/}}] is to be replaced by the output type of the function to be constructed (if the value that follows the return command is not of this type, a implicit type conversion will be attempted). This part (together with the \lq\code{->}\rq) can be omitted if the output type will be clear from the context.
\item[\textcolor{commentgray}{\code{/*algorithm*/}}] is to be replaced by the algorithm calculating the return value from the parameters and the captured variables.
\end{description}
an easy example of a definition of a \code{std::function} via the \cc lambda calculus could look like this:
\begin{lstlisting}
int i = 5;
std::function f<double(int)> = [i](const int& n) {i*log(n);};
\end{lstlisting}
Here the output type was omitted, since it is specified in the \code{std::function}.

We remark, that members can not be captured directly: if \code c is an object of an class \code C, which has a member k of type T, then
\begin{lstlisting}
[c.k]() -> T {return c.k;};
\end{lstlisting}
will not work. The reason is the following: Assume the class C has some member function f changing the value of k. In this case
\begin{lstlisting}
[&c, c.k]() -> T {c.f(); return c.k;};
\end{lstlisting}
would be ambiguous, since c.k could mean both the field of the object c captured by reference, which has the new value, or the member c.k, which was captured by copy before the change was made and therefore should have the old value.

Thus we will have to first copy the member to a local variable, then capture it.

\subsection{\code{std::shared\_ptr}s}

A shared pointer is an object consisting of an pointer (to an object of specified type) and a pointer to an reference counter. Each time the shared pointer is copied the reference counter will be increased. If a shared pointer is destroyed, it decreases the reference counter and checks if it is zero, and if it is, it destroys the object it is pointing to.

Shared pointers are very useful for treelike ownership relations: If one object is used by multiple other objects, each of the latter can, instead of an pointer or a copy, to the former hold a shared pointer. This way it is guaranteed, that we neither have useless copies, nor memory leaks because no one feels responsible for destroying.

The \code{std::shared\_ptr} can be dereferenced by a preceding $*$, just as a regular pointer, can be constructed from regular pointers and compared to the \NULL. Here is an short example:

\begin{lstlisting}
double* ptr = new double(4.5);
std::shared_ptr<double> s_ptr(ptr);
cout << *s_ptr << endl;
\end{lstlisting}
returns \lq 4.5\rq.

There are two main sources of errors when handling shared pointers:
\begin{itemize}
\item Multiple shared pointers are constructed from the same pointer: In this case each shared pointer keeps its own reference counter. If the first of those counters hits zero, the object will be destroyed. If now a shared pointer following an other reference counter tries to access the object, there will be an error.
\item Circular ownership relations: for simplicity lets have a look at the case of two lonely shared pointers pointing at each other. Each of the pointers has a reference counter which is one (since they are lonely). now assume we destroy one of them. This will decrease the corresponding reference counter and, since the reference count will hit zero, destroy the other shared pointer. This in turn will decrease its reference counter, which will also hit zero. Therefore it will attempt to destroy the first shared pointer. Which is already destroyed. This will lead to an error.
\end{itemize}

\section{Some classes of functions and similar objects}

Before we talk about the class of Taylor series we are aiming to implement, we will introduce two more general classes (and one more restrictive). These classes are convenient for us, since a lot of code, which would otherwise make the class of Taylor series huge, can be \lq exported\rq. This improves code readability. Of course we also hope that these classes might proof useful on their own.

Before we take a closer look at the individual classes, we give a short overview:
\begin{description}
\item[\func{POLY<coeff\_type>}] will be a minimalistic class for polynomials with coefficients of type \code{coeff\_type}.
\item[\func{FUNC<RESULT(PARAM)>}] will be a new class for functions. Its functionalities will be very similar to those of \ir{FUCNTION<PARAM, RESULT>}, but the implementation will be somewhat different: It will heavily be relying on the tools provided by the \ccOx standard template library.
\item[\func{POWERSERIES<coeff\_type>}] will implement formal power series. This is, functions from the integers to objects of type \code{coeff\_type} but with the convolution replacing the point wise product and the function \code{get\_coeff} to avoid the operator \code{()}, which in this case might be ambiguous, since it could mean both the evaluation as function from the integers to \code{coeff\_type} or analytic function. Also a formal derivative and anti derivative will be implemented.
\end{description}

There will be one additional helper class \func{coeff\_fetcher<coeff\_type>}, to improve the speed at which the coefficients of \func{POWERSERIES} can be evaluated.

To keep the class definitions presented in the following subsections readable, they might differ slightly from those, that can be found in the code. \temp{Some of the details left out will be addressed in later sections.}

\subsection{The class \func{POLY}}

This class is supposed to model polynomials with coefficients from some class \code{coeff\_type}. Its main component is a shared pointer named \code{coeff}, which points to a constant \code{std::vector<coeff\_type>}. The vector is a constant, so will not have to copy it if we copy the polynomial. We will just copy the shared pointer. If we need to change the coefficients, we will create a new vector and reassign the shared pointer. If the polynomial was the only one referencing the former vector, this vector will be automatically deleted. The class definition of \code{POLY} looks similar to this:

\begin{lstlisting}
template<class coeff_type>
class POLY {
	// members:
		private:
			shared_ptr<const vector<coeff_type>> coeff;
	// constructors:
		public:
			POLY();
			template<class T>
			POLY(const T&);
			POLY(vector<coeff_type>);
	// standard operators:
		public:
			POLY& operator = (const POLY<coeff_type>&);
			friend POLY operator - (const POLY<coeff_type>&);
			/* ... */
			POLY& operator *= (const POLY<coeff_type>&);
	// member functions:
		public:
			coeff_type get_coeff(const unsigned int n) const;
			unsigned int get_degree() const;
			template<class ARG>
			ARG operator () (const ARG&);
};
\end{lstlisting}

We briefly discuss the general purpose of the components of this class, before we address some of the key implementations.
\begin{description}
\item[\code{members}:] The one existent member has already been discussed above.
\item[\code{constructors}:] In line 8 the empty constructor can be found, it will construct the zero polynomial. Then there is a constructor template in line 9 and 10, which will attempt to convert any given type \code{T} to \code{coeff\_type} and then construct the polynomial having this value as first and only coefficient. Finally in line 11 we find a constructor, that constructs a polynomial from a given list of coefficients. We refrain from handing the vector over as a reference to allow syntax such as \code{POLY<REAL> P(\{1,2,pi()\})}, where \code{\{1,2,pi()\}} as a temporary object can not be referenced.
\item[\code{standard operators}:] There is no complete list of the operators above, so we give one here: The operators \code{=} (copy constructor), \code{-} (additive inverse), \code{+}, \code{-} (substraction), \code{*}, \code{+=} and \code{*=} are overloaded for polynomials.
\item[\code{member functions}:] The function \code{get\_degree()} will return the length of the coefficient vector. The highest coefficient might be zero, so we do not really get the degree, but a upper bound for it. Since we can not check whether a coefficient is zero in general, this is not avoidable. \code{get\_coeff(n)} will return the \code{n}-th coefficient, if \code{n} is bigger than the coefficient vector is long, it will return zero. Another option would be to issue a warning or even an error, but the chosen approach simplifies the implementations of the standard operators. line 22 and 23 are taken by an evaluation operator. Since polynomials are often evaluated in types more general than \code{coeff\_type} (for example Polynomials with integer coefficients are regularly taken as functions on the real or complex numbers) this operator is a template. Of course the argument \code{ARG} needs to provide an addition and multiplication. Additionally \code{ARG} needs to have an multiplication with \code{coeff\_type} from the right (this is in particular the case if \code{ARG} is constructible from \code{coeff\_type}).
\end{description}

The implementation of the constructors are straight forward. So are most of the implementations of the standard operators. As example we take a closer look at the addition:
\begin{lstlisting}
POLY operator + (const POLY<coeff_type>& P) {
	unsigned int max_degree = max(get_degree(), P.get_degree());
	vector<coeff_type> new_coeff(max_degree + 1);
	for (unsigned int i = 0; i <= max_degree; i++) {
		new_coeff[i] = get_coeff(i) + P.get_coeff(i);
	}
	return POLY(new_coeff);
}
\end{lstlisting}
Here we first calculate the maximum of the degrees and construct a vector of the right size. Then we calculate the new coefficients as sum of the old ones. Note that the function \code{get\_coeff} saves us one case distinction: It automatically adds zeros to the end of the smaller polynomial. We pay for the convenience by the additional time consumption of adding zeros.

\subsection{The class \func{FUNC}}

This class is supposed to model (computable) functions which map objects of some class \code{PARAM} to objects of some other class \code{RESULT}. It will mainly consist of an shared pointer \code{algorithm} to a \code{std::function<RESULT(const PARAM\&)>} we will refer to as the algorithm of the function. Since \code{algorithm} will be passed on, if a new function is created from existing ones, it will point to a constant. Changing the algorithm will be done by creating a new function and reassigning \code{algorithm}. If the algorithm is not used by any other function, it will automatically be removed. We remark that, for improved syntax, the class \func{FUNC} is a template specialization (compare to the end of \cref{sec: std::functions}).
\begin{lstlisting}
template<class PARAM, class RESULT>
class FUNC<RESULT(PARAM)> {
	// members:
		protected:
			shared_ptr<const function<RESULT(const PARAM&)> algorithm;
	// constructors:
		public:
			FUNC(const alg_func<PARAM, RESULT>& f);
		protected:
			FUNC();
	// standard operators:
		public:
			FUNC& operator = (const FUNC<RESULT(PARAM)>&);
			FUNC operator = (const function<RESULT(PARAM)>&);
			friend FUNC<RESULT(PARAM)> operator - <> (const FUNC<RESULT(PARAM)>&);
			/*...*/
			friend FUNC<RESULT(PARAM)> operator - <> (const FUNC<RESULT(PARAM)>&,const FUNC<RESULT(PARAM)>&);
			template<class PAR>
			FUNC<RESULT(PAR)> operator () (const FUNC<PARAM(PAR)>&);
	// member functions:
		public:
			RESULT operator () (const PARAM&) const;
};
\end{lstlisting}
We will first discuss the general purpose of the components of this class, and then address some of the key implementations.
\begin{description}
\item[\code{members}:] In line 5 the main component of the class can be found: The shard pointer to the algorithm of the function.
\item[\code{constructors}:] There are two main constructors: The one listed in line 8 will construct a \func{FUNC} from an algorithm. There is an empty constructor in line 10. This constructor is protected, since a FUNC without an algorithm is pretty much useless. But it might be convenient to have an empty constructor accessible for the derived classes.
\item[\code{standard operators}:] All the standard operators will be overloaded for \func{FUNC}s. For improved symmetry the operators are external and thus represented by friend templates in the class definition. The composition is a exception to this rule: Since it is not possible to define new operators, the composition (line 22) will have the syntax $f(g)$ instead of $f\circ g$. This requires it to be a member of \func{FUNC}, since the operator \code{()} has to be.
\item[\code{member functions}:] The only member function is the evaluation, which should be self-explanatory.
\end{description}

The implementations of this class are very straight forward. We will only take a look at the composition, as an example of how to combine shared pointers to \code{std::function}s and the \cc lambda calculus:
\begin{lstlisting}
template<class PARAM, class RESULT>
template<class PAR>
FUNC<RESULT(PAR)> FUNC<RESULT(PARAM)>::operator () (
	const FUNC<PARAM(PAR)>& f
) {
	if((algorithm == NULL)||(f.algorithm == NULL))
		return FUNC<RESULT(PAR)>();
	alg_ptr<PARAM, RESULT> _algorithm = algorithm;
	alg_ptr<PAR, PARAM> f_algorithm = f.algorithm;
	alg_ptr<PAR, RESULT> new_algorithm(new const auto function(
		[_algorithm, f_algorithm](const PAR& x) -> RESULT {
			return (*_algorithm)((*f_algorithm)(x));
		}
	));
	return FUNC<RESULT(PAR)>(new_algorithm);
}
\end{lstlisting}
Here we used the abbreviation \code{alg\_ptr<PARAM, RESULT>} for \code{shared\_ptr<const function<RESULT(const PARAM\&)>}. First we check, that both the functions have algorithms (line 6). If one of them does not, we return a function without an algorithm. Next we save the algorithms of both functions to local variables (compare the end of \cref{sec: The cc lambda calculus}). In line 10 we define a new function via the \cc lambda calculus. The lambda expression captures the local variables we defined by copy and returns upon input the composition of the two algorithms. Since the new algorithm owns shared pointers to the algorithms of the functions to be composed, these algorithms will not be deleted before the new algorithm is destroyed. Finally in line 8 we return a function with the composition of the algorithms as algorithm.


\subsection{The class \func{POWERSERIES}}

The class \func{POWERSERIES} is supposed to model formal power series. As such it is a template in the coefficient type, abbreviated by \code{c\_t}. We want to be able to compute sums and products of power series, therefore the coefficient type needs to have an addition and an multiplication defined. More specific \code{c\_t} needs to be a unital ring in the sense, that the corresponding standard operators need to be defined (constructors from positive integers, $+$, $*$, additive inverse). 
\begin{lstlisting}
template<class c_t>
class POWERSERIES {
	// members:
		private:
			shared_ptr<const FUNC<c_t(const unsigned int&)>> coeff;
			coeff_fetcher<c_t>* get_coeffs=new coeff_fetcher<c_t>(this);
	// constructors:
		public:
				POWERSERIES(c_t);
				POWERSERIES(FUNC<c_t(const unsigned int&)>);
				POWERSERIES(function<c_t(const unsigned int&)>);
	// standard operators:
		public:
			POWERSERIES& operator = (const POWERSERIES<c_t>&);
			POWERSERIES operator + (const POWERSERIES<c_t>&) const;
			/*...*/
			POWERSERIES operator * (const POWERSERIES<c_t>&) const;
			POWERSERIES operator () (const POWERSERIES<c_t>&) const; 
	// member functions:
		public:
			c_t get_coeff(const unsigned int&) const;
			POLY<c_t> cut_of_at(const unsigned int&) const;
			void derive(const unsigned int&);
			void anti_derive(const unsigned int&);	
\end{lstlisting}
We describe its components:
\begin{description}
\item[\code{members}:] The main component of a \func{POWERSERIES} is a shared pointer to a sequence, which we represent by an object of type \func{FUNC<c\_t(const unsigned int\&)>}, that is a function from the positive integers to the coefficient type. The second member, an pointer to an object of type \lq coeff\_fetcher\rq\ will be addressed at the end of this subsection, and is merely for performance improvement.
\item[\code{constructors}:] There are three constructors. The first (line 9) takes an object \code{x} of the coefficient type and returns the power series with first coefficient \code{x} followed by zeros (This power series represents the constant function which always returns \code{x}). The second constructor takes what we decided to represent a sequence and returns the corresponding power series. The third one can be directly fed a \cc lambda expression.
\item[\code{standard operators}:] The standard operators are overloaded for power series. We again emphasise, that the multiplication is the convolution and not point wise. For the composition to be well defined, the first coefficient of the inner power series needs to be zero. Since \code{c\_t} can for example be the type \ir{REAL}, for which a test of equality is not accessible, the composition will not give an warning, if the first coefficient of the inner series is not zero, but the return value will be incorrect.
\item[\code{member functions}:] The member functions should be self explanatory.
\end{description}

The implementations of this class are very straight forward, and can easily be understood from the source code (assuming, that one has read the example implementations of the classes before).

\subsubsection{The class \code{coeff\_fetcher}}

The class \code{coeff\_fetcher} is a helper class to enable caching of the coefficient values, that also works in constant POWERSERIES. Since the class \code{coeff\_fetcher} is only a helper class and should not be available to the user, it will be put in a anonymous namespace.

To explain the use of the class coeff\_fetcher, consider the product \code{P*Q} of two power series \code{P} and \code{Q}. Each time we call \code{(P*Q).get\_coeff(n)}, the convolution of the power series is calculated, which involves \code{n} multiplications and \code{n} additions. There are cases, where the same coefficient is needed multiple times, for example if we want to evaluate the powers \code{P}$^i$ of some power series. In this case it would be more efficient, to remember the coefficients and not to recompute them each time.

The above can be implemented by saving a vector \code{known\_coeffs} of pointers to the coefficient type: Each time some, say the \code{n}-th coefficient is requested, the function \code{get\_coeff} will check, whether \code{known\_coeffs[n]} is defined and not the \NULL. If it is, it will return the value pointed to. If it is not, it calculate the value and set \code{known\_coeffs[n]} to point to a coefficient type of the result value.

Thus far, this could be implemented in the class \func{POWERSERIES} itself. There is one problem: It is often reasonable to work with constant power series (for example the power series underlying a \func{BASE\_ANALYTIC} should be constant for the exact same reasons the algorithm of a \func{FUNC} and the sequence of a \func{POWERSERIES} is). But in this case, the vector \code{known\_coeffs}, as a member of a constant object, will also be constant. Thus this vector will be stored in a external class \func{coeff\_fetcher}, and be referenced in \func{POWERSERIES} only by pointer.

We remark, that the class \func{coeff\_fetcher} does have one additional member function \code{reset()}, to reset the vector of known values to an empty vector. This is for the case, that saving values leads to memory overflow.

\section{A class for Taylor series}

\subsection{The class definition}
\subsection{The evaluation}

\end{document}